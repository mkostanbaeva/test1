SWARM 
This is a set of containers - http_headers, http_request, http_response, Url, url_query. There are containers with a set of fields (exept URL), in case http_headers container which consisting of a list of key - value. 

URL - a special class that represents the API to work with links. It uses for working the libcurl library. Every time when called a seturl method or constructor, it saved an original cURL at the internal structure in a special field. On the first request produced lazy initialization  to any of the fields except this curl (other than the original field). That is, called a special methods from libcurl to parse which came the line, to check its validity and then put in the fields. When calling to_string method occurs a lazy parsing a URL (if you haven't done so already), then use libcurl methods after what made generation from units. 
Parsing in libcurl is a very strict, it does not allow to be any deviation. So, if in url is something wrong, it is considered invalid. The fromuserinput method was made for url, which was entered by the user or from an external application.It verifies the url and try to make valid. If unable to do it, returns FALSE. 

SWARM_XML library
urlfinder
The goal of this class is get the input xml and return a list of url.The goal of this class is get the input xml and return a list of url. Currently used in xmlpath. It uses a special api to parse html documents. It searches for all the tags that return url and adds its contents to the list of url (without any manipulation). 
urlfatcher is a wrapper over liburl with the properties:
- @eventloop - a special object that allows you to manage and monitor various events associated with timers and input-output devices. In particular, there is a class eventloop, which is an abstraction over @eventloop (based boost_asio and libev). It should create a socket with specified types of network domain, protocol (to be able to close), listen to the events of the socket - read operations, write operations, request call timer. Libev - more or less canonical library, on which basis was tested URL. From libev library uses a timer, async, ev_ill. Whenever the url invokes the command, it wants to listen to these events. It checks whether created for this socket ev_ill, if not - it creates. If you ask it to kill - it kills. 
The cURL may ask to stop listening socket at eventloop, or listen to it read-only or write-only , or read and write. When it says to do something new, it must forget everything talking about ever before. We considered a normal cURL - call it at the specified socket, there is some action, it calls inside these does something, and then ask to close this socket. Here arise problems - what you need to remember that bad to kill within a method that finds. It destroys the stack.
Timer. When curl asks eventloop create a timer, you should not forget about all the previous timers. Previous timers it forgets and calls only the last.
Host- cURL it himself never calls, but is necessary for reasons based on the curl is not multithreaded. Api all that it has to be guaranteed called from the same thread. Now because of this surfaced strange problems when working with the curl in different streams within each stream was created their instance curl and had frequent requests https this leads to accidental drops inside openSSL.
ev_eventloop timer uses when requesting evtimer, when requesting to post event is used ev-async + special queue of callbacks. When a thread of the stream we want to call the new method then blocked list of callbacks. Then variant - a new event, and then write the log, and then tell it to async something do. Then it will call evlibev special callback in its stream that will run through all the events and all will be called in turn.
Boost_asio developers did not try to cross to curl. The first problem - if you take the old version of curl, there curl could not ask eventloop create its socket or kill the socket, it has always created his avd and it passed. Boost library can not work with the avd and just allow to work with it, the library requires that owned it. Therefore boosteventlop has to create a copy of that the avd and to work with her. It retains all the properties of the parent, except that curl does not close the socket for it. Need to close both avd to closed the socket. When CURL is requesting to hang some event on the socket, then evenlupe need to check what we know about this awd or see it for the first time. If the first time, it should duplexes it and create a boost socket. If you know, then take out the existing sockets created a boost socket and hang it on the event to listen. In later versions, we can listen to curl, there are methods to create, open socket, then we can immediately create the boost socket, and curl give any AWD for which can be uniquely identified the boost socket. This greatly simplifies things. Due to this, required to check implementations the boost eventloop in both cases.
When cURL requests to listen socket for reading, it actually asks you do not say when it will be read from this socket in the near future, from now on, it says always in the socket when there are new data. It reads in the paradigm libev library, but quite contrary boost_asio library. When boost_asio library can read the data - it calls once, and then again need to push over data. Libev library asks to close the socket inside the call this socket. Always needs to remember the last of its condition what it asked to do with this socket.This imposes certain restrictions on how to prolong the life of this socket. That is, when we ask to stop listening socket, we should stop listening to him but not to close and not to say to curl any data from this socket. But at other times, when you call the close, we can not close it instantly because it can be inside a call to this socket.Every time when we are asked to create a socket , we create its shared pointer. We're losing it at closing the socket. In all treatments of the socket we take shared pointer and something doing with it. When event is called that we read the new data and need to remember a weak pointer. Then check the socket is not dead (that is a weak pointer Null or not Null) when the event is called. Trying transfer shared pointer and if it done, we can do something. This gives a more stable job with curl. That is socket kills only when we are asked to kill, and all the events that arrive after that check whether they are a socket or not. 
Post events to the eventloop stream are produced through the usual method of despatch from ioservice. Timer made ​​through deadlinetimer boost_asio liblary and in it for each request timer must cancel a previous request and then make a new request. Curl can ask to call it in 0 seconds sometimes. We should call as soon as possible, but not right now. Usually this is done so in boost_asio - into the event queue put a curl call a new event and it will be caused by itself, when it returns to eventloop. That is already coming out of curl, coming from all methods, leave everywhere, then take out the event from eventloop we call the appropriate cURL method. 
Urlfatcher tying itself over curl. Problem - curl single-threaded, and events from urlfatcher would like to throw in another thread. So each call of get and post methods that has accompanied this dispatch call eventloop in running curl. Inside urlfatcher can not work with any libev or boost_asio library. When it comes to some event of the arguments get or post formed a special event structure and sends handler of this event structure into a stream eventloop. Next, try to create a curl request for this request. Create curl object for processing this request. After convert our represantation into lists of hidder of curl. Attached data (if post) and the data from curl, but curl does not copy the data. Therefore requires to put somewhere this line of data that not to lose. After should to convert curl in line and transmit curl, redefine a commands to create the socket, to close the socket, redefine callbacks, which tells us from cURL that came header and a data.
Next, tries to create an cURL-easy object, add it to cURL-multi object and it will tell us whether it has create or not. If do not do it, throw an error. For this is using callinforedirectcount (?) - special field that says how many times the curl got redirects. If the number is changes, that try to deceive us and we are in the following request. Then it need to forget about all that headers are arrived and again begin to fill some sort of fields. In this, we do not know that it was the last line with the header.
We need to know the error code, that the headers is ended and then can to call some user method. Therefore, it is verified that came the two values ​​of /n. 
End of request processing - the cURL has a special message queue which need to check from time to time. There may be any messages, but now stored only the completion message of the query. Depending on the message you can send a message about the status of the query to the user. After that will killed itself cURL-easy  object and carefully pull out of its cURL-multi object. So it's all consistent with the idea of ​​cURL and nothing was destroyed. If you've finished processing the event. In urlfatcher have another special property - we can send many simultaneous requests. But cULR can not do it. Therefore, there is a check on the amount of active compounds - the number of simultaneous requests. And at the end, when we processed next message - it is necessary to check whether we can send the next request. For this supported the special request queue, which has not yet been able to send because there is a limit. This bypasses the trouble to work with cURL.
